{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original filename: \"02_mpof_diversion_amounts.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following commands in python to install these packages\n",
    "#pip install statsmodels\n",
    "#pip install loess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import calendar\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess as lw\n",
    "from loess.loess_1d import loess_1d as ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths, input, and output file names\n",
    "\n",
    "outer_dir = os.path.abspath(os.path.join(os.getcwd() ,\"../..\"))\n",
    "refpth = os.path.join(outer_dir, 'IFT_files', 'Reference Files')\n",
    "sfe_char_csv = os.path.join(refpth, 'TNC SFE LOI Characteristics with MAF.csv')\n",
    "sfe_char_xls= os.path.join(refpth, 'TNC SFE LOI Characteristics.xlsx')\n",
    "subset_xls = os.path.join(refpth, 'TNC Subset SFE LOIs.xlsx')\n",
    "#sfe_char_csv = os.path.join(refpth, 'All SFE LOI Characteristics with MAF.csv')\n",
    "#sfe_char_xls= os.path.join(refpth, 'All SFE LOI Characteristics.xlsx')\n",
    "#subset_xls = os.path.join(refpth, 'Subset SFE LOIs.xlsx')\n",
    "\n",
    "unimpath = os.path.join(outer_dir, \"IFT_files\", 'Unimpaired Flow')\n",
    "comid_csv = os.path.join(refpth, 'TNC-POI-COMID-20210804.csv')\n",
    "#comid_csv = os.path.join(refpth, 'SFER-POI-COMID-16Jun2020.csv')\n",
    "wytdir = os.path.join(unimpath, \"Water Year Types\")\n",
    "wmtdir = os.path.join(unimpath, \"Water Month Types\")\n",
    "\n",
    "startdir = os.path.join(outer_dir, 'IFT_files', 'IFT Results')\n",
    "wmtfile = os.path.join(wmtdir, \"LOI 9999 WMT.csv\") #file containing WMTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgwin = 30 #window size of moving average\n",
    "switchdays = 30 # number of days to look from beginning and end of list of dates to determine when to switch between LOESS and moving average\n",
    "\n",
    "startdate = dt.datetime(1950,10,1)\n",
    "stopdate = dt.datetime(2021,8,1)\n",
    "\n",
    "#startdate = dt.datetime(1995,10,1)\n",
    "#stopdate = dt.datetime(2017,9,30)\n",
    "alldates = pd.date_range(startdate,stopdate)\n",
    "\n",
    "# The following variables can have a single value or a list of values (i.e., [#1, #2, #3,...]). The script loops through\n",
    "# however many values are put in\n",
    "exd_perc_flows = [0.1] #[0.1, 0.2, 0.3] #the exceedance percentile flow to create the streamflow baseline (default = 10% or 0.1)\n",
    "divert_ratios = [0.1, 0.2] #[0.1, 0.2, 0.3] #proportion of streamflow baseline for setting diversion allocation (default = 10% or 0.1)\n",
    "prd_pct_reqts = [0.1] #percentile of the length of time the requirement applies, as specified in reqt_time. For example,\n",
    "# if this is 0.1, the 10th percentile of all flows\n",
    "reqt_times = [0] #[0,1,2,3] #length of time for requirements to apply. 0 for daily, 1 for weekly, 2 for semi-monthly (1st and 15th), 3 for monthly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original filename: \"00_add_ts_col.py\"\n",
    "def add_ts_col(tab):\n",
    "#adds column for TS as first column of DataFrame as formatted date\n",
    "    tab['TS'] = tab.index.strftime('%m/%d/%Y')\n",
    "    cols = tab.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "    tab = tab[cols]\n",
    "    return tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original filename: \"00_get_all_sfe_lois.py\"\n",
    "# Read in SFE LOI characteristics table and calculate bankfull flow using cont. Area relation\n",
    "\n",
    "def get_all_sfe_lois():\n",
    "    sfelois = pd.read_excel(sfe_char_xls,index_col=0)\n",
    "    loi = [str(i) for i in sfelois['LOI']]\n",
    "    sfelois['Outlet LOI'] = sfelois['LOI']\n",
    "    sfelois['Contributing Area (mi^2)'] = sfelois['Contributing Area']\n",
    "    #sfelois['MAF'] = sfelois['Mean Annual Flow (cfs)']\n",
    "    sfelois['Qbf'] = 71.5 * sfelois['Contributing Area (mi^2)'] #71.5 cfs/mi^2 according to Darren\n",
    "\n",
    "    subset = pd.read_excel(subset_xls)\n",
    "    sublois = sfelois.loc[subset['SWSID'],:]\n",
    "    return sublois, loi, sfelois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original filename: \"00_read_loi_paradigm_flow_v3.py\"\n",
    "def read_loi_paradigm_flow(p):\n",
    "    unimpflowfile = os.path.join(unimpath, str(p) + '.csv')\n",
    "    unimp = pd.read_csv(unimpflowfile,index_col=0)\n",
    "    unimp.index = pd.DatetimeIndex(unimp.index)\n",
    "    return unimp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sep-01-2021 09:27:12 AM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\kklausmeyer\\AppData\\Local\\Continuum\\anaconda3\\envs\\freshwater\\lib\\site-packages\\pandas\\core\\indexing.py:1027: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return getattr(section, self.name)[new_key]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPOF - Default Complete\n",
      "MPOF - 20% of 10th Percentile Hydrograph, Daily Complete\n",
      "Sep-01-2021 09:27:18 AM\n"
     ]
    }
   ],
   "source": [
    "loitab, loi, fultab = get_all_sfe_lois()\n",
    "print(dt.datetime.now().strftime('%b-%d-%Y %I:%M:%S %p'))\n",
    "#loop through all selections\n",
    "for e in exd_perc_flows:\n",
    "    for d in divert_ratios:\n",
    "        for r in reqt_times:\n",
    "            for p in prd_pct_reqts:\n",
    "                mpofout = pd.DataFrame(columns=loi, index=alldates)\n",
    "                if (not r == 0) or ((r == 0) and (p == prd_pct_reqts[len(prd_pct_reqts)-1])): #don't do daily for every different p\n",
    "                    #determine MPOF variation labels\n",
    "                    exdstr = str(int(e * 100)) + 'th Percentile Hydrograph, '\n",
    "                    pctstr = str(int(d * 100)) + '%'\n",
    "                    prdstr = str(int(p * 100)) + 'th Percentile of '\n",
    "                    if r == 0:\n",
    "                        lenstr = 'Daily'\n",
    "                        prdstr = ''\n",
    "                    elif r == 1:\n",
    "                        lenstr = 'Weekly'\n",
    "                    elif r == 2:\n",
    "                        lenstr = 'Biweekly' #Note: not perfectly biweekly, but new requirements set on the 1st and 15th of each month\n",
    "                    elif r == 3:\n",
    "                        lenstr = 'Monthly'\n",
    "                    methstr = pctstr+ ' of ' +exdstr+ prdstr+lenstr\n",
    "                    if (e == 0.1) & (d == 0.1) & (r == 0):\n",
    "                        methstr = 'Default'\n",
    "                    methstr = 'MPOF - ' + methstr\n",
    "                    for l in loi:\n",
    "                        unimp = read_loi_paradigm_flow(l) #read in unimpaired flow\n",
    "\n",
    "                        # determine exceedance hydrograph from unimpaired flow\n",
    "                        flow90exd = unimp['flow'].groupby(by=[unimp.index.month, unimp.index.day]).quantile(e)\n",
    "                        #fix for water year order of months\n",
    "                        prevyr = flow90exd.loc[np.arange(10,13)]\n",
    "                        flow90exd = prevyr.append(flow90exd.loc[np.arange(1, 10)])\n",
    "\n",
    "                        tempind = [dt.datetime(1995,10,1)+dt.timedelta(x) for x in range(366)]\n",
    "                        flow90exd.index = tempind\n",
    "                        loessind = flow90exd.index\n",
    "                        loessfilt = lw(flow90exd.get_values(), loessind, frac=70 / 365, it=0) #70-day loess filter\n",
    "\n",
    "                        # moving average\n",
    "                        flowmovavg= flow90exd.append(flow90exd)\n",
    "                        mastr = str(avgwin) + '-Day MA'\n",
    "                        movavg_dups = flowmovavg.rolling(avgwin,center=True).mean().dropna().sort_index() #calculate rolling/moving avg\n",
    "                        movavg = movavg_dups.loc[~movavg_dups.index.duplicated(keep='first')] #remove duplicates\n",
    "                        #format into table\n",
    "                        flowbase = pd.DataFrame({'Daily 90% Exceedance': flow90exd, mastr: movavg, 'LOESS': loessfilt[:,1]},index=flow90exd.index)\n",
    "                        methdiff = np.abs(flowbase[mastr]-flowbase['LOESS']) #difference between moving avg and loess\n",
    "                        #determine day within beginning and ending number of days to swtich from loess to moving avg (default = 30)\n",
    "                        headswitch = methdiff.index[1:switchdays+1][methdiff.iloc[1:switchdays+1] == min(methdiff.iloc[1:switchdays+1])]\n",
    "                        tailswitch = methdiff.index[-(switchdays+1):-1][methdiff.iloc[-(switchdays+1):-1] == min(methdiff.iloc[-(switchdays+1):-1])]\n",
    "                        #determine streamflow baseline from combination of methods\n",
    "                        flowbase['Daily Streamflow Baseline (cfs)'] = flowbase.loc[min(flowbase.index):pd.DatetimeIndex(headswitch-pd.DateOffset(1)).to_pydatetime()[0],mastr].append(\n",
    "                            flowbase.loc[headswitch.to_pydatetime()[0]:(tailswitch-pd.DateOffset(1)).to_pydatetime()[0],'LOESS']).append(\n",
    "                            flowbase.loc[tailswitch.to_pydatetime()[0]:max(flowbase.index),mastr])\n",
    "\n",
    "\n",
    "                        if r == 0: #daily, no resampling needed\n",
    "                            reindfb = flowbase.copy()\n",
    "                        else:\n",
    "                            if r == 1: # weekly\n",
    "                                freqdates = [min(flowbase.index) + (dt.timedelta(days=7) * i) for i in range(0, 53)]\n",
    "                                gbinds = np.zeros_like(flowbase.index,dtype='int')\n",
    "                                for i in range(0, len(freqdates)):\n",
    "                                    gbinds[(flowbase.index < freqdates[i]) & (gbinds == 0)] = i\n",
    "                                gbinds[gbinds == 0] = 53\n",
    "                                indfq = 'W'\n",
    "                            elif r == 2: #biweekly\n",
    "                                days = np.array([1,15])\n",
    "                                months = np.append(np.arange(10,13),np.arange(1, 10))\n",
    "                                years = np.append(1995 * np.ones(6), 1996 * np.ones(18)).astype(int) ## NOT sure why 1996 and 1995\n",
    "                                dates = np.array(np.meshgrid(months, days)).T.reshape(-1, 2)\n",
    "                                freqdates = [dt.datetime(years[i],dates[i,0], dates[i,1]) for i in range(len(dates))]\n",
    "                                gbinds = np.zeros_like(flowbase.index,dtype='int')\n",
    "                                for i in range(0, len(freqdates)):\n",
    "                                    gbinds[(flowbase.index < freqdates[i]) & (gbinds == 0)] = i\n",
    "                                gbinds[gbinds == 0] = 24\n",
    "                                indfq = 'SMS'\n",
    "                            elif r == 3: #monthly\n",
    "                                gbinds = flowbase.index.month\n",
    "                                indfq = 'MS'\n",
    "                            reindfb = flowbase.groupby(gbinds, sort=False).quantile(p) #group by time period and take quantile\n",
    "                            reindfb.index = pd.date_range(min(tempind), max(tempind), freq=indfq)\n",
    "                            reindfb = reindfb.reindex(flowbase.index, method='ffill')\n",
    "                        #calculate diverion allocation, both daily and resampled (may be the same if both are daily)\n",
    "                        flowbase['Daily Diversion Allocation (cfs)'] = d * flowbase['Daily Streamflow Baseline (cfs)']\n",
    "                        flowbase['Resampled Streamflow Baseline (cfs)'] = reindfb['Daily Streamflow Baseline (cfs)']\n",
    "                        flowbase['Resampled Diversion Allocation (cfs)'] = d * reindfb['Daily Streamflow Baseline (cfs)']\n",
    "\n",
    "                        #save off calculations performed so results can be analyzed manually\n",
    "                        if not(os.path.exists(os.path.join(startdir, methstr))):\n",
    "                            os.mkdir(os.path.join(startdir, methstr))\n",
    "                        flowbase.to_csv(os.path.join(startdir, methstr, 'LOI ' + l + ' MPOF ' + methstr + ' Data.csv'))\n",
    "\n",
    "                        #now need to take calculated diversion allocation and subtract it  from unimpaired flow to get IFT\n",
    "                        fbst = flowbase.copy()\n",
    "                        fbst.index = flowbase.index.strftime('%m-%d')\n",
    "                        years = np.arange(min(alldates.year), max(alldates.year)+1)\n",
    "                        #need to copy diversion allocation to all years\n",
    "                        fballyr = pd.concat([fbst]*(years[-1]-years[0]))\n",
    "                        alyind = (np.concatenate([np.repeat(years[0],92), \\\n",
    "                                                  np.repeat(years[1:-1],366), \\\n",
    "                                                  np.repeat(years[-1],274)]) \\\n",
    "                                  .astype(str)+fballyr.index)\n",
    "                        fballyr.index = alyind\n",
    "                        for y in years[1:]:\n",
    "                            if ~calendar.isleap(y):\n",
    "                                fballyr.drop(index=str(y)+'02-29',inplace=True) #remove feb 29 where it doesn't exist\n",
    "                        fballyr.index = pd.to_datetime(fballyr.index,format='%Y%m-%d')\n",
    "                        mpofout.loc[mpofout.index,l] = unimp.loc[mpofout.index,'flow'] - fballyr.loc[mpofout.index,'Resampled Diversion Allocation (cfs)']\n",
    "\n",
    "                        mpofout.loc[mpofout[l] < 0, l] = 0 #can't have negative IFT\n",
    "                    #format for WEAP and save\n",
    "                    mpofout = add_ts_col(mpofout)\n",
    "                    savdir = startdir\n",
    "                    if not ((e == 0.1) & (d == 0.1) & (r == 0)):\n",
    "                        savdir = os.path.join(startdir, 'MPOF Variants')\n",
    "                        if not(os.path.exists(savdir)):\n",
    "                            os.mkdir(savdir)\n",
    "                    mpofout.to_csv(os.path.join(savdir, 'All LOI ' +methstr+ ' IFTs.csv'))\n",
    "                    print(methstr + ' Complete')\n",
    "\n",
    "print(dt.datetime.now().strftime('%b-%d-%Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1995-10-01    0.007401\n",
       "1995-10-02    0.007737\n",
       "1995-10-03    0.009812\n",
       "1995-10-04    0.010198\n",
       "1995-10-05    0.010198\n",
       "1995-10-06    0.011036\n",
       "1995-10-07    0.009771\n",
       "1995-10-08    0.010078\n",
       "1995-10-09    0.010198\n",
       "1995-10-10    0.012858\n",
       "1995-10-11    0.013441\n",
       "1995-10-12    0.013664\n",
       "1995-10-13    0.020356\n",
       "1995-10-14    0.020643\n",
       "1995-10-15    0.021201\n",
       "1995-10-16    0.021026\n",
       "1995-10-17    0.020969\n",
       "1995-10-18    0.021083\n",
       "1995-10-19    0.025838\n",
       "1995-10-20    0.027868\n",
       "1995-10-21    0.027160\n",
       "1995-10-22    0.026529\n",
       "1995-10-23    0.038648\n",
       "1995-10-24    0.040341\n",
       "1995-10-25    0.039093\n",
       "1995-10-26    0.034009\n",
       "1995-10-27    0.069899\n",
       "1995-10-28    0.069988\n",
       "1995-10-29    0.071332\n",
       "1995-10-30    0.071749\n",
       "                ...   \n",
       "1996-09-01    0.008944\n",
       "1996-09-02    0.008154\n",
       "1996-09-03    0.010463\n",
       "1996-09-04    0.009770\n",
       "1996-09-05    0.009435\n",
       "1996-09-06    0.008789\n",
       "1996-09-07    0.007772\n",
       "1996-09-08    0.009042\n",
       "1996-09-09    0.007400\n",
       "1996-09-10    0.007191\n",
       "1996-09-11    0.007126\n",
       "1996-09-12    0.005738\n",
       "1996-09-13    0.005493\n",
       "1996-09-14    0.005436\n",
       "1996-09-15    0.004806\n",
       "1996-09-16    0.009188\n",
       "1996-09-17    0.010071\n",
       "1996-09-18    0.009608\n",
       "1996-09-19    0.009770\n",
       "1996-09-20    0.010739\n",
       "1996-09-21    0.008969\n",
       "1996-09-22    0.007062\n",
       "1996-09-23    0.006708\n",
       "1996-09-24    0.008714\n",
       "1996-09-25    0.008865\n",
       "1996-09-26    0.008154\n",
       "1996-09-27    0.007328\n",
       "1996-09-28    0.006405\n",
       "1996-09-29    0.011270\n",
       "1996-09-30    0.010223\n",
       "Name: flow, Length: 366, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow90exd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['1950-10-01', '1950-10-02', '1950-10-03', '1950-10-04',\n",
       "               '1950-10-05', '1950-10-06', '1950-10-07', '1950-10-08',\n",
       "               '1950-10-09', '1950-10-10',\n",
       "               ...\n",
       "               '2021-07-23', '2021-07-24', '2021-07-25', '2021-07-26',\n",
       "               '2021-07-27', '2021-07-28', '2021-07-29', '2021-07-30',\n",
       "               '2021-07-31', '2021-08-01'],\n",
       "              dtype='datetime64[ns]', length=25873, freq='D')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alldates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (freshwater)",
   "language": "python",
   "name": "freshwater"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
